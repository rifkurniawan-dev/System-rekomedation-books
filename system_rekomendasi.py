# -*- coding: utf-8 -*-
"""system rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CFQe-1H6X9oNE48BIhZ6ZxDJYc5mFa4C

# **Kumpulan Data Rekomendasi buku**

## **Data Loading**

### **Download**
"""

!pip install scikit-surprise
!pip install numpy==1.23.5
!pip install tensorflow==2.12.0

"""### **Import Library**

melakukan import pada beberapa libraries, seperti pandas, numpy, tenserflow, dll.
"""

import pandas as pd
import numpy as np

from surprise import accuracy,SVD, Dataset, Reader
from surprise.model_selection import train_test_split
from surprise.model_selection import cross_validate

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt

"""Pada tahap ini, kita akan memilih file yang berbentuk csv yang akan diolah. Pada proyek ini kita hanya menggunakan file ```Books.csv```, ```Ratings.csv``` dan ```Users.csv```. Langkah awal kita hubungankan colab ke drive kita dengan fungsi mount"""

#Menghubungkan colab ke drive
from google.colab import drive
drive.mount('/content/drive')

"""tahap selanjutnya yaitu memanggil dataset dari drive yang telah di hubungkan mengggunakan fungsi ```mount``` dengan mengistall ```gdown``` sebelum dilakukan pemanggilan"""

#install gdown
!pip install gdown

"""Tahap selanjutnya memasukan link yang di dalamnya berupa dataset yang telah di copy dari drive ke colab"""

#memasukan link dataset
import gdown
file_id = '10QXZ7pb1kcv8gfmgDz6jisc3BaunowBC'
url = f'https://drive.google.com/uc?id={file_id}'
gdown.download(url, output='rekomendasi_buku.zip', quiet=False)

"""setekah selesai di masukan langkah selanjutnya mengestrak file yang berupa zip"""

# Ekstrak file zip
import zipfile

with zipfile.ZipFile('rekomendasi_buku.zip', 'r') as zip_ref:
    zip_ref.extractall('rekomendasi_buku')

"""selanjutnya mengecek nama kelas yang telah di ekstrak"""

# Cek nama kelas
import os

dataset_path = 'rekomendasi_buku'
class_names = os.listdir(dataset_path)
print("Kelas yang ditemukan:", class_names)

"""Berdasarkan kelas yang di temukan pada dataset ini ada 6 kelas yaitu ```classicRec.png```, ```Users.csv```, ```DeepRec.png```, ```Ratings.csv```, ```Books.csv``` dan ```recsys_taxonomy2.png```.
Namun data kelas yang di perlukan pada projek system rekomendasi berupa file csv sehingga file yang bukan csv tidak di perlukan

# **Exploratory Data Analysis (EDA)**

## **Data Understanding**

## **Gathering Data**

Pada projek ini, kita hanya akan menggunakan dataframe ```book```, ```rating``` dan ```users```. melihat jumlah masing-masing dataframe ketiganya
"""

#melihat jumlah kolom dan baris dari masing-masing dataframe
books = pd.read_csv('rekomendasi_buku/Books.csv', low_memory=False)
ratings = pd.read_csv('rekomendasi_buku/Ratings.csv', low_memory=False)
users = pd.read_csv('rekomendasi_buku/Users.csv', low_memory=False)
print(books.shape)
print(ratings.shape)
print(users.shape)

"""Berdasarkan dataset yang ditemukan informasi yaitu :

* Data book memiliki 8 kolom dan 271,360
* Data rating memiliki 3 kolom dan 1,149,780
* Data user memiliki 3 kolom dan 278,858

Membuat dataframe baru untuk menggabungkan ketiga dataset dengan nama mergee_df
"""

#menggabungkan ke tiga data
mergee_df = pd.merge(ratings, books, on='ISBN')
mergee_df = pd.merge(mergee_df, users, on='User-ID')
mergee_df

"""menampilakan lima data paling atas dari ketiga dataset tersebut menggunakan fungsi ```head()```."""

#menampilkan isi awal data
books.head()

"""Setelah menjalankan kode di atas, dapat dilihat bahwa pada dataset ```Books.csv ``` memiliki kolom:

* ISBN: Kode unik dari sebuah buku
* Book-Title: Judul buku
* Book-Author: Pengarang buku
* Year-Of-Publication: Tahun terbit buku
* Publisher: Penerbit buku
* Image-URL-S: Cover buku berukuran kecil
* Image-URL-M: Cover buku berukuran sedang
* Image-URL-L: Cover buku berukuran besar
"""

ratings.head()

"""Setelah menjalankan kode di atas, dapat dilihat bahwa dataset ```Ratings.csv``` memiliki kolom:

* User-ID: Kode unik dari pengguna yang memberikan penilaian
* ISBN: Kode unik dari sebuah buku
* Book-Rating: Penilaian buku nilai terendah dimulai dari angka 0
"""

users.head()

"""Setelah menjalankan kode di atas, dapat dilihat bahwa dataset Users.csv memiliki kolom:

* User-ID: Kode unik dari pengguna
* Location: Lokasi pengguna
* Age: Usia pengguna

Berdasarkan dataset yang dilihat dari ```Users.csv``` mengalami banyak missing value yang akan ditangani selanjutnya. Sebelum itu, mari kita lihat jumlah keseluruhan buku, rating, dan user pada proyek kali ini.
"""

print('Jumlah keseluruhan buku: ', len(books.ISBN.unique()))
print('Jumlah keseluruhan rating: ', len(ratings[['ISBN', 'User-ID']].drop_duplicates()))
print('Jumlah keseluruhan user: ', len(users['User-ID'].unique()))

"""mengetahui informasi penting seperti tipe pada data"""

books.info()

"""Setelah mengeksekusi kode di atas, dapat dilihat bahwa seluruh kolom pada dataset ```Books.csv``` memiliki tipe yang berupa data *object*. Ada hal unik yang didapati ketika menjalankan kode di atas, dapat dilihat bahwa kolom ```Year-Of-Publication``` bertipe data object sedangkan tahun publikasi umumnya bertipe data ```integer```, oleh karena itu perlu adanya perbaikan pada tipe data.

### Mengubah Tipe Data
"""

books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Membuang value pada ```Year-Of-Publication``` ada yang bernilai ```DK Publishing Inc``` dan ```Gallimard```."""

temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Mengubah tipe data pada ```Year-Of-Publication``` dari ```object``` ke ```integer```."""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

ratings.info()

"""Setelah menjalankan kode di atas, dapat dilihat bahwa kolom User-ID dan Book-Rating pada dataset ```Ratings.csv``` memiliki tipe data ```integer ``` sedangkan kolom ISBN bertipe ```object```."""

users.info()

"""Setelah menjalankan kode di atas, dapat dilihat bahwa kolom-kolom pada ```Ratings.csv``` memiliki tipe data sebagai berikut:

* User-ID bertipe integer
* Location bertipe object
* Age bertipe desimal

## Univariate Data Analysis

melakukan distribusi pada dataframe
"""

author_counts = books.groupby('Book-Author')['Book-Title'].count()
sorted_authors = author_counts.sort_values(ascending=False)
top_20_authors = sorted_authors.head(20)
plt.figure(figsize=(12, 6))
top_20_authors.plot(kind='bar')
plt.xlabel('Nama Penulis')
plt.ylabel('Jumlah Buku')
plt.title('20 Penulis Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Setelah menjalankan kode di atas dapat dilihat bahwa Penulis Agatha Christie menulis paling banyak buku yaitu sebanyak > 600 buku."""

publisher_counts = books.groupby('Publisher')['Publisher'].count()
sorted_publisher = publisher_counts.sort_values(ascending=False)
top_20_publisher = sorted_publisher.head(20)
plt.figure(figsize=(12, 6))
top_20_publisher.plot(kind='bar')
plt.xlabel('Nama Penerbit')
plt.ylabel('Jumlah Buku')
plt.title('20 Penerbit Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Setelah menjalankan kode di atas dapat dilihat bahwa Penerbit Harlequin menerbitkan paling banyak buku yaitu sebanyak > 7000 buku."""

year_of_publication_counts = books.groupby('Year-Of-Publication')['Year-Of-Publication'].count()
sorted_year_of_publication = year_of_publication_counts.sort_values(ascending=False)
top_20_year_of_publication = sorted_year_of_publication.head(20)
plt.figure(figsize=(12, 6))
top_20_year_of_publication.plot(kind='bar')
plt.xlabel('Tahun Terbit')
plt.ylabel('Jumlah Buku')
plt.title('20 Tahun Terbit Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Setelah menjalankan kode di atas dapat dilihat bahwa Tahun Terbit paling banyak menerbitkan buku yaitu pada tahun 2002 sebesar > 17.500 buku."""

plt.figure(figsize=(8, 6))
plt.hist(ratings['Book-Rating'], bins=10, edgecolor='black')
plt.xlabel('Rating Buku (0-10)')
plt.ylabel('Jumlah Buku')
plt.title('Distribusi Rating Buku')
plt.show()

"""Setelah menjalankan kode di atas dapat dilihat bahwa rating buku sebagian besar mendapaptkan rating 0."""

# Hitung rata-rata rating dari setiap judul buku
mean_rating_by_title = mergee_df.groupby('Book-Title')['Book-Rating'].mean()
top_20_titles = mean_rating_by_title.nlargest(20)
top_20_titles

"""Setelah menjalankan kode di atas dapat dilihat bahwa berikut merupakan 20 judul buku dengan rata-rata rating terbesar."""

# Hitung rata-rata rating dari setiap judul buku
max_rating_by_title = mergee_df.groupby('Book-Title')['Book-Rating'].mean()
# Pilih 20 judul buku dengan rata-rata rating terkecil
top_20_titles = max_rating_by_title.nsmallest(20)
top_20_titles

"""Setelah menjalankan kode di atas dapat dilihat bahwa berikut merupakan 20 judul buku dengan rata-rata rating terkecil.

# **Data Preprocessing**

Pada tahap Data Preprocessing dilakukan 2 langkah yaitu Penggabungan Data dan Hapus Variabel yang Tidak Digunakan
"""

books.head()

ratings.head()

mergee_df = ratings.merge(books, on='ISBN')
mergee_df.head(10)

"""# **Hapus Variabel Yang Tidak Digunakan**
Menghapus variabel-variabel yang tidak digunakan di pengembangan proyek dengan motode collaborative filtering yaitu

1. Image-URL-S
2. Image-URL-M
3. Image-URL-L
4. Book-Title
5. Book-Author
6. Year-Of-Publication
7. Publisher
"""

from sys import breakpointhook
column_to_drop = ['Image-URL-S','Image-URL-M','Image-URL-L','Book-Title','Book-Author','Year-Of-Publication','Publisher']
book_rating_df = mergee_df.drop(column_to_drop, axis =1)
book_rating_df.head()

"""Data Preparation
Pada tahap ini akan dilakukan langkah-langkah sebagai berikut:

* Missing Value
* Encoding
* Skala Ulang Variabel Book-Rating
* Pembagian Data Uji dan Data Latih

### **Missing Value**
"""

ratings.isnull().sum()

"""Tidak ditemukan missing value pada dataset

### **Encoding Data**

Dilakukan encoding ID untuk ISBN dan User-ID ke dalam indeks integer
"""

user_ids = book_rating_df['User-ID'].unique().tolist()
user_encoded = {x: i for i, x in enumerate(user_ids)}
user_to_user_encoded = {i: x for i, x in enumerate(user_ids)}

book_ids = book_rating_df['ISBN'].unique().tolist()
book_encoded = {x: i for i, x in enumerate(book_ids)}
book_to_book_encoded = {i: x for i, x in enumerate(book_ids)}

print('Encoded user-id :', dict(list(user_encoded.items())[0: 10]))
print('Encoded book-id (isbn) :', dict(list(book_encoded.items())[0: 10]))

"""setelah itu memetakan data hasil encoding ke dataframe dengan menambahkan kolom baru yaitu kolom user_id dan isbn"""

book_rating_df['user_id'] = book_rating_df['User-ID'].map(user_encoded)
book_rating_df['isbn'] = book_rating_df['ISBN'].map(book_encoded)
book_rating_df.head()

"""### **Skala Ulang Variabel Book-Rating**
Dilakukan normalisai terhadap variabel Book-Rating yaitu merubah nilai rating menjadi nilai dengan batas skala 0 sampai 1 agar mudah dalam proses training

Mengubah nilai Book-Rating menjadi float
"""

col_rating = 'Book-Rating'
book_rating_df[col_rating] = book_rating_df[col_rating].values.astype(np.float32)

"""Mengambil nilai min dan max Book-Rating


"""

min_rating = min(book_rating_df[col_rating])
max_rating = max(book_rating_df[col_rating])

"""Menambahkan variabel baru di dataframe dengan nama rating


"""

book_rating_df['rating'] = book_rating_df[col_rating].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
book_rating_df.head()

"""### **Pembagian Data Latih dan Data Uji**
Membagi data menjadi data uji dan data latih dengan perbandingan 80% data latih dan 20% data uji.

Pembagian data uji dan data latih ini akan dilakukan dengan 2 cara yang berbeda. Hal ini dikarenakan untuk teknik SVD pembagian data uji dan data latih menggunakan modul ```train_test_split``` dari library suprise. Berikut masing-masing cara pembagian data uji dan lati.

* Pembagian data uji dan latih untuk teknik SVD
"""

reader_svd = Reader(rating_scale=(0,1))
data_svd = Dataset.load_from_df(book_rating_df[['user_id', 'isbn', 'rating']], reader_svd)
trainset_svd, testset_svd = train_test_split(data_svd, test_size=0.2)

#Pembagian data uji dan latih untuk teknik Neural Network

x = book_rating_df[['user_id','isbn']].values
y = book_rating_df['rating'].values

n_train = 0.8
train_indices = int(n_train * book_rating_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# **Modeling**

Dalam proyek ini, sistem rekomendasi dibangun menggunakan pendekatan collaborative filtering. Dua teknik yang diterapkan dalam pendekatan ini adalah:

* Collaborative filtering menggunakan metode Singular Value Decomposition (SVD)

* Collaborative filtering berbasis Neural Network

Untuk keperluan pengujian model, dibuatlah data uji berupa daftar buku yang belum pernah dibaca oleh pengguna berdasarkan User-ID yang dipilih.
"""

test_user_id = 276729
read_books = ratings[ratings['User-ID'] == test_user_id]
unread_books =  ratings[~ratings['ISBN'].isin(read_books.ISBN.values)]['ISBN']

"""Singular Value Decomposition (SVD) adalah metode dari aljabar linear yang dimanfaatkan untuk mereduksi dimensi matriks. Melalui pendekatan ini, hubungan atau pola tersembunyi antar elemen dalam matriks dapat diidentifikasi."""

svd = SVD()

svd.fit(trainset_svd)
df_svd_predict = unread_books.to_frame(name='ISBN')
df_svd_predict = df_svd_predict.merge(books[['ISBN','Book-Title','Book-Author']], left_on='ISBN', right_on='ISBN', how='left')
df_svd_predict['prediction_rate'] = unread_books.apply(lambda x: svd.predict(test_user_id, x).est)

n_top = 10
df_svd_predict.sort_values(by='prediction_rate',ascending=False).head(n_top)

"""### **Collaborative Filtering dengan Pendekatan Neural Network**
Pendekatan collaborative filtering berbasis Neural Network ini diadaptasi dari contoh pada situs Keras yang menggunakan kelas RecommenderNet.

Langkah awalnya adalah membuat kelas RecommenderNet yang diturunkan dari kelas Model milik Keras. Model ini dirancang untuk menghitung match score antara pengguna dan buku melalui operasi dot product. Nilai tersebut kemudian disesuaikan dengan menambahkan user bias dan book bias. Sebagai langkah akhir, skor yang diperoleh dinormalisasi ke dalam rentang 0 hingga 1 menggunakan fungsi aktivasi sigmoid.


"""

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.book_embedding = layers.Embedding(
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_books, 1)
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    book_vector = self.book_embedding(inputs[:, 1])
    book_bias = self.book_bias(inputs[:, 1])

    dot_user_resto = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_resto + user_bias + book_bias

    return tf.nn.sigmoid(x)

"""* Model dikembangkan dengan menggunakan fungsi kehilangan (loss function) Binary Crossentropy, yang sesuai untuk memodelkan output dalam bentuk probabilitas. Untuk proses optimisasi, digunakan algoritma Adam (Adaptive Moment Estimation) yang dikenal efisien dalam menangani data berdimensi besar dan pelatihan yang kompleks. Sebagai metrik evaluasi, digunakan Root Mean Square Error (RMSE) untuk mengukur seberapa besar selisih antara prediksi dan nilai sebenarnya.

* Hasil prediksi model teknik Neural Network untuk 10 buku yang disarankan
"""

embedding_size = 50
num_users = len(book_rating_df['user_id'])
num_books = len(book_rating_df['isbn'])

model_nn = RecommenderNet(num_users, num_books, embedding_size)
model_nn.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

unread_isbns = list(set(unread_books).intersection(set(book_encoded.keys())))
unread_isbns_encode = [[(book_encoded.get(x))] for x in unread_isbns]

user_encoder = user_encoded.get(test_user_id)
user_book_array =  np.hstack(([[user_encoder]] * len(unread_isbns_encode), unread_isbns_encode))

pred_ratings = model_nn.predict(user_book_array).flatten()

op_ratings_indices = pred_ratings.argsort()[-n_top:][::-1]
recommended_book_isbns = [book_to_book_encoded.get(unread_isbns_encode[x][0]) for x in op_ratings_indices]
df_nn_predict = books[books['ISBN'].isin(recommended_book_isbns)]
df_nn_predict[['ISBN','Book-Title','Book-Author']]

"""# **Evaluation**
Evaluasi model menggunakan metriks RMSE (Root Mean Square Error)
"""

rmse = pd.DataFrame(columns=['test'], index=['SVD','NeuralNetwork'])

"""Evaluasi model SVD dilakukan menggunakan modul accuracy dari library Surprise untuk mengukur performa prediksi terhadap data uji."""

predictions = svd.test(testset_svd)
rmse.loc['SVD','test'] = accuracy.rmse(predictions)

"""Evaluasi collaborative filtering untuk teknik Neural Network


"""

history = model_nn.fit(
    x = x_train,
    y = y_train,
    batch_size = 500,
    epochs = 4,
    validation_data = (x_val, y_val)
)

plt.figure(figsize=(8, 5))
plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Performa Model Berdasarkan RMSE')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error (RMSE)')
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()

rmse.loc['NeuralNetwork','test'] = history.history['val_root_mean_squared_error'][-1]
rmse

"""Dilihat dari tabel di atas, terlihat bahwa nilai RMSE dari teknik Neural Network lebih besar dibandingkan dengan teknik SVD. Meskipun selisihnya tidak terlalu signifikan, hasil ini menunjukkan bahwa performa model Neural Network masih dapat ditingkatkan lebih lanjut dengan melakukan tuning hyperparameter yang lebih optimal, seperti penyesuaian jumlah neuron, learning rate, dan jumlah epoch.

Oleh karena itu, meskipun saat ini teknik SVD menunjukkan hasil yang lebih akurat, teknik Neural Network tetap dipilih sebagai algoritma sistem rekomendasi buku dalam proyek ini, karena memiliki potensi yang lebih besar untuk dikembangkan dan ditingkatkan kinerjanya ke depannya.

### **Conclusion**
Sistem rekomendasi buku telah berhasil dikembangkan menggunakan metode collaborative filtering dengan pendekatan Neural Network. Teknik ini dipilih berdasarkan nilai RMSE yang relatif kecil dan potensi pengembangan lebih lanjut.

Meskipun hasil yang diperoleh sudah cukup baik, proyek ini masih memiliki ruang untuk perbaikan, khususnya pada aspek kualitas dataset serta penyesuaian hyperparameter pada model Neural Network agar performa sistem rekomendasi dapat lebih ditingkatkan di masa mendatang.
"""